#!/usr/bin/env python3;
"""
Robust accuracy evaluation & confusion‑matrix generator for EyeLog.

Features
========
* Pairs predicted coverage CSV (generated by `run_accuracy_test.py` / GUI) with ground‑truth CSV per video (`frame_number,label`).
* Computes confusion‑matrix, accuracy, precision, recall, F1.
* Saves per‑video confusion‑matrix PNG + JSON classification report.
* Aggregates all video metrics into a single JSON.
* Multi‑process for speed.

Usage
-----
```
python accuracy_evaluator.py \
    --logs-dir test_results \
    --annotations-dir annotations \
    --out-dir reports \
    --coverage-threshold 5.0
```
Dependencies: `numpy`, `scikit-learn`, `matplotlib`. Install with:
```
pip install -r requirements.txt
```
`requirements.txt`:
```
numpy
scikit-learn
matplotlib
```
"""

import argparse, glob, json, os, csv, logging, multiprocessing as mp
from pathlib import Path
from typing import Tuple

import numpy as np
from sklearn.metrics import (confusion_matrix, classification_report,
                             accuracy_score)
import matplotlib
matplotlib.use("Agg")
import matplotlib.pyplot as plt

logging.basicConfig(level=logging.INFO,
                    format="%(asctime)s | %(levelname)s | %(message)s")

# -----------------------------------------------------------------------------
# Helpers
# -----------------------------------------------------------------------------

def read_predictions(csv_path: Path, threshold: float) -> Tuple[np.ndarray, np.ndarray]:
    """Return (frame_numbers, predicted_labels) where label=1 if coverage>=threshold."""
    frames, labels = [], []
    with csv_path.open(newline="") as f:
        reader = csv.DictReader(f)
        for row in reader:
            frames.append(int(row["frame_number"]))
            cov = float(row["coverage_percent"])
            labels.append(1 if cov >= threshold else 0)
    return np.asarray(frames), np.asarray(labels)


def read_ground_truth(csv_path: Path) -> Tuple[np.ndarray, np.ndarray]:
    """Return (frame_numbers, true_labels)."""
    frames, labels = [], []
    with csv_path.open(newline="") as f:
        reader = csv.DictReader(f)
        for row in reader:
            frames.append(int(row["frame_number"]))
            labels.append(int(row["label"]))
    return np.asarray(frames), np.asarray(labels)


def align_by_frames(fr_pred, lbl_pred, fr_gt, lbl_gt):
    """Align arrays by shared frame_numbers; returns (y_pred, y_true)."""
    common, idx_pred, idx_gt = np.intersect1d(fr_pred, fr_gt, return_indices=True)
    if common.size == 0:
        raise ValueError("No overlapping frames between prediction and GT: %s vs %s" % (fr_pred.size, fr_gt.size))
    return lbl_pred[idx_pred], lbl_gt[idx_gt]


def save_confusion_plot(cm: np.ndarray, out_png: Path, class_names=("NoMaterial", "Material")):
    fig, ax = plt.subplots(figsize=(4, 4), dpi=120)
    im = ax.imshow(cm, interpolation="nearest")
    ax.set(xticks=range(len(class_names)), yticks=range(len(class_names)),
           xticklabels=class_names, yticklabels=class_names,
           ylabel="True label", xlabel="Predicted label")
    plt.setp(ax.get_xticklabels(), rotation=45, ha="right", rotation_mode="anchor")
    thresh = cm.max() / 2.0
    for i in range(cm.shape[0]):
        for j in range(cm.shape[1]):
            ax.text(j, i, format(cm[i, j], "d"), ha="center", va="center",
                    color="white" if cm[i, j] > thresh else "black")
    fig.tight_layout()
    fig.savefig(out_png)
    plt.close(fig)


def evaluate_pair(pred_csv: Path, gt_csv: Path, out_dir: Path, thr: float):
    logging.info(f"Evaluating {pred_csv.name} vs {gt_csv.name}")
    fr_pred, lbl_pred = read_predictions(pred_csv, thr)
    fr_gt, lbl_gt = read_ground_truth(gt_csv)
    try:
        y_pred, y_true = align_by_frames(fr_pred, lbl_pred, fr_gt, lbl_gt)
    except ValueError as e:
        logging.warning(str(e))
        return None

    cm = confusion_matrix(y_true, y_pred)
    report_dict = classification_report(y_true, y_pred, output_dict=True)
    acc = accuracy_score(y_true, y_pred)

    stem = pred_csv.stem
    out_png = out_dir / f"{stem}_cm.png"
    save_confusion_plot(cm, out_png)

    json_path = out_dir / f"{stem}_report.json"
    with json_path.open("w") as f:
        json.dump(report_dict, f, indent=2)

    return {"file": pred_csv.name, "accuracy": acc, "report": report_dict}


def worker(args):
    return evaluate_pair(*args)

# -----------------------------------------------------------------------------
# Main
# -----------------------------------------------------------------------------

def main():
    parser = argparse.ArgumentParser(description="EyeLog accuracy evaluator")
    parser.add_argument("--logs-dir", required=True)
    parser.add_argument("--annotations-dir", required=True)
    parser.add_argument("--out-dir", default="reports")
    parser.add_argument("--coverage-threshold", type=float, default=5.0,
                        help="Coverage %% threshold to flag presence of material")
    parser.add_argument("--n-jobs", type=int, default=mp.cpu_count())
    args = parser.parse_args()

    logs_dir = Path(args.logs_dir)
    ann_dir = Path(args.annotations_dir)
    out_dir = Path(args.out_dir)
    out_dir.mkdir(parents=True, exist_ok=True)

    pred_files = list(logs_dir.glob("*.csv"))
    tasks = []
    for pred_csv in pred_files:
        base = pred_csv.stem.split("__")[0]  # video name convention
        gt_csv = ann_dir / f"{base}.csv"
        if not gt_csv.exists():
            logging.warning(f"GT for {pred_csv.name} not found → skip")
            continue
        tasks.append((pred_csv, gt_csv, out_dir, args.coverage_threshold))

    if not tasks:
        logging.error("No matching prediction/GT pairs found.")
        return

    with mp.Pool(processes=args.n_jobs) as pool:
        results = [res for res in pool.map(worker, tasks) if res]

    if results:
        (out_dir / "aggregate_metrics.json").write_text(json.dumps(results, indent=2))
        logging.info(f"Reports saved to {out_dir.resolve()}")


if __name__ == "__main__":
    main()

